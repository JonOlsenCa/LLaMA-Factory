# VGPT2 v4 Stage 1: Supervised Fine-Tuning (SFT)
# ==============================================
# OPTIMAL APPROACH: SQLCoder base model + Schema-in-Prompt format
#
# Key Changes from v3:
# 1. Base model: defog/llama-3-sqlcoder-8b (SQL-specialized)
# 2. Dataset: vgpt2_v4_sft (SQLCoder-style DDL-in-prompt)
# 3. Quality over quantity: ~3,000 curated examples vs 67K auto-generated
#
# Hardware: RTX A6000 (48GB) + 128GB RAM + Threadripper 7960X
# Expected training time: ~30-45 min for 3 epochs (optimized)
#
# Usage:
#   llamafactory-cli train automation/configs/vgpt2_v4/stage1_sft.yaml

### Model Settings ###
model_name_or_path: defog/llama-3-sqlcoder-8b
trust_remote_code: true
flash_attn: fa2                      # Flash Attention 2 - 2x faster than SDPA

### Windows Single-GPU Settings ###
use_unsloth: false                   # Disabled - breaks LlamaFactory deps
torch_compile: false                 # Disabled - causes freezing on Windows

### Fine-tuning Method ###
finetuning_type: lora
lora_rank: 128                       # Sufficient for 3K examples
lora_alpha: 256                      # 2x lora_rank
lora_dropout: 0.05
lora_target: all                     # Target all linear layers

### Dataset Settings ###
dataset: vgpt2_v4_sft_expanded       # SQLCoder-style DDL-in-prompt format (expanded)
template: llama3                     # LLaMA 3 template for SQLCoder
cutoff_len: 4096                     # SQLCoder uses 4K context
max_samples: 10000                   # Allow full dataset + expansion
overwrite_cache: true
preprocessing_num_workers: 1         # Single worker - avoids Windows multiprocessing issues
dataloader_num_workers: 0            # MUST be 0 on Windows
packing: false                       # DISABLED - was causing training collapse

### Training Settings ###
stage: sft
do_train: true
per_device_train_batch_size: 4       # Reduced for stability
gradient_accumulation_steps: 4       # Effective batch size = 16 (same)
learning_rate: 1e-4                  # Reduced - 2e-4 may have caused collapse
num_train_epochs: 3.0                # 3 epochs for small curated dataset
lr_scheduler_type: cosine
warmup_ratio: 0.1                    # 10% warmup for small dataset
bf16: true
gradient_checkpointing: true         # Enable for memory efficiency
gradient_checkpointing_kwargs: '{"use_reentrant": false}'

### Output Settings ###
output_dir: saves/vgpt2_v4/sft_optimized
logging_steps: 10
save_steps: 200                      # Less frequent saves - reduces overhead
save_total_limit: 3                  # Keep last 3 checkpoints
plot_loss: true
overwrite_output_dir: true           # Fresh start
save_only_model: true                # Faster saves - skip optimizer state
report_to: wandb                     # W&B tracking (installed by 00_setup.ps1)
run_name: vgpt2_v4_sft_optimized

### Resume Training ###
resume_from_checkpoint: null         # Fresh start

### Evaluation ###
val_size: 0.1                        # 10% holdout (~300 samples)
do_eval: true
eval_strategy: steps
eval_steps: 200                      # Less frequent evals
per_device_eval_batch_size: 8
