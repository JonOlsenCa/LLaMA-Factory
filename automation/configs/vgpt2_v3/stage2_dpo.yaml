# VGPT2 v3 Stage 2: Direct Preference Optimization (DPO)
# ======================================================
# Teaches model to prefer correct SQL over incorrect SQL
# Uses preference pairs: chosen (correct) vs rejected (incorrect)
#
# Hardware: RTX A6000 (48GB) + 128GB RAM + Threadripper 7960X
# Expected training time: 4-6 hours for 2 epochs
#
# Prerequisites:
#   - Complete Stage 1 SFT training first
#   - Generate DPO preference pairs dataset
#
# Usage:
#   llamafactory-cli train automation/configs/vgpt2_v3/stage2_dpo.yaml

### Model Settings ###
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
adapter_name_or_path: saves/vgpt2_v3/sft    # Load SFT adapter
trust_remote_code: true

### Fine-tuning Method ###
stage: dpo
do_train: true
finetuning_type: lora
lora_rank: 256
lora_alpha: 512
lora_dropout: 0.05
lora_target: all

### DPO Settings ###
pref_beta: 0.1                       # KL penalty coefficient
pref_loss: sigmoid                   # Standard DPO loss (options: sigmoid, orpo, simpo)

### Dataset Settings ###
dataset: vgpt2_v3_dpo                # 5K+ preference pairs
template: qwen
cutoff_len: 4096                     # Shorter for preference pairs
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 1
dataloader_num_workers: 0

### Training Settings ###
per_device_train_batch_size: 2       # DPO needs more memory per sample
gradient_accumulation_steps: 8      # Effective batch size = 16
learning_rate: 5.0e-6                # Much lower LR for DPO
num_train_epochs: 2.0                # 2 epochs sufficient for preference learning
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
gradient_checkpointing: true

### Output Settings ###
output_dir: saves/vgpt2_v3/dpo
logging_steps: 10
save_steps: 200
save_total_limit: 3
plot_loss: true
overwrite_output_dir: false
save_only_model: false
report_to: none

### Resume Training ###
resume_from_checkpoint: false

### Evaluation ###
val_size: 0.05                       # 5% holdout
do_eval: true
eval_strategy: steps
eval_steps: 200
per_device_eval_batch_size: 1
