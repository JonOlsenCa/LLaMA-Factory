# VGPT2 v3 Model Option: Qwen2.5-7B Full Fine-Tune
# ================================================
# All parameters updated - deepest learning possible
# Requires DeepSpeed ZeRO-3 for memory efficiency
#
# Hardware: RTX A6000 (48GB) with CPU offloading
# Expected training time: 24-36 hours for 3 epochs
#
# Usage:
#   llamafactory-cli train automation/configs/vgpt2_v3/model_options/qwen7b_full.yaml

### Model Settings ###
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
trust_remote_code: true

### Fine-tuning Method ###
stage: sft
do_train: true
finetuning_type: full                # Full fine-tuning (all 7.9B params)
deepspeed: examples/deepspeed/ds_z3_config.json  # ZeRO-3 for memory

### Dataset Settings ###
dataset: vgpt2_v3_sft
template: qwen
cutoff_len: 4096                     # Reduced for full fine-tune memory
max_samples: 100000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 0
packing: false

### Training Settings ###
per_device_train_batch_size: 1       # Minimal for full fine-tune
gradient_accumulation_steps: 16      # Effective batch size = 16
learning_rate: 1.0e-5                # Much lower LR for full fine-tune
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.05
bf16: true
ddp_timeout: 180000000

### Output Settings ###
output_dir: saves/vgpt2_v3_full/sft
logging_steps: 10
save_steps: 500
save_total_limit: 3                  # Full checkpoints are large!
plot_loss: true
overwrite_output_dir: false
save_only_model: true                # Only save model to reduce disk usage
report_to: none

### Resume Training ###
resume_from_checkpoint: false

### Evaluation ###
val_size: 0.02
do_eval: true
eval_strategy: steps
eval_steps: 500
per_device_eval_batch_size: 1
